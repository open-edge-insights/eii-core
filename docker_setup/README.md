
Docker compose setup of ETA solution:
=====================================
--------------------------------------

The setup instructions below assume the OS is installed and ready.
If Clearlinux is used, please follow the [docker_setup/clear_linux_setup_guide.md](clear_linux_setup_guide.md) for installing the Clearlinux OS.

## <u>Pre-requisities</u>:

### **`Ubuntu Only`** 

1. Install latest docker cli/docker daemon by following [https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce](https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce). Follow **Install using the repository** and **Install Docker CE (follow first 2 steps)** sections there. Also, follow the manage docker as a non-root user section at [https://docs.docker.com/install/linux/linux-postinstall/](https://docs.docker.com/install/linux/linux-postinstall/) to run docker without sudo

2. Please follow the below steps only if the node/system on which the docker setup is tried out is running behind a HTTP proxy server. If that's not the case, this step can be skipped.
    
    * Configure proxy settings for docker client to connect to internet and for containers to access internet by following [https://docs.docker.com/network/proxy/](https://docs.docker.com/network/proxy/). One can copy the below json object to ~/.docker/config.json (**Note**: `Depending on the geo location where the system is setup, please use the proxy settings of that geo. This change may not be needed in  non-proxy environment`):

        ```
        {
            "proxies":
                {
                    "default":
                    {
                        "httpProxy": "http://proxy.iind.intel.com:911",
                        "httpsProxy": "http://proxy.iind.intel.com:911",
                        "noProxy": "127.0.0.1,localhost,*.intel.com"
                    }
                }
        }
        ```

    * Configure proxy settings for docker daemon by following the steps at [https://docs.docker.com/config/daemon/systemd/#httphttps-proxy](https://docs.docker.com/config/daemon/systemd/#httphttps-proxy). Use the values for http proxy and https proxy as used above (**Note**: `Depending on the geo location where the system is setup, please use the proxy settings of that geo. This change may not be needed in  non-proxy environment`)

    * If you still see issues of not being able to access internet from container, please update `resolv.conf` file at `docker_setup/resolv.conf`. The `docker_setup/compose_startup.sh` and `docker_setup/deploy/deploy_compose_startup.sh` scripts will re-copy the resolv.conf to `/etc/resolv.conf` to keep it updated across system restarts (**Note**: `This change may not be needed in non-proxy environment`):

        ```
        A. Ubuntu 16.04 and earlier

            For Ubuntu 16.04 and earlier, /etc/resolv.conf was dynamically generated by NetworkManager.

            Comment out the line dns=dnsmasq (with a #) in /etc/NetworkManager/NetworkManager.conf

            Restart the NetworkManager to regenerate /etc/resolv.conf :
            sudo systemctl restart network-manager

            Verify on the host: cat /etc/resolv.conf

        B. Ubuntu 18.04 and later

            Ubuntu 18.04 changed to use systemd-resolved to generate /etc/resolv.conf. Now by default it uses a local DNS cache 127.0.0.53. That will not work inside a container, so Docker will default to Google's 8.8.8.8 DNS server, which may break for people behind a firewall.

            /etc/resolv.conf is actually a symlink (ls -l /etc/resolv.conf) which points to /run/systemd/resolve/stub-resolv.conf (127.0.0.53) by default in Ubuntu 18.04.

            Just change the symlink to point to /run/systemd/resolve/resolv.conf, which lists the real DNS servers:
            sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf

            Verify on the host: cat /etc/resolv.conf
        ```

3. Install `docker-compose` tool by following this [https://docs.docker.com/compose/install/#install-compose](https://docs.docker.com/compose/install/#install-compose)


### **`Ubuntu and ClearLinux`**

1. Copy the Yumei acceptance test videos to the `test_videos` folder under `ElephantTrunkArch` by using the following commands:

    ```
	cd ElephantTrunkArch/test_videos
	wget -q http://wheeljack.ch.intel.com/test_videos/defect-video1.avi
	wget -q http://wheeljack.ch.intel.com/test_videos/defect-vid2.avi
	wget -q http://wheeljack.ch.intel.com/test_videos/defect-vid3.avi
    ```

2. Clone the a locally maintained [kapacitor repository](https://github.intel.com/ElephantTrunkArch/kapacitor) inside the `ElephantTrunkArch` folder by obtaining the command from gerrit/teamforge

    **NOTE**: Please use the git repo of kapacitor as is, the script `build.py` is dependent on that.

3. Since docker compose setup publishes ports to host and ia_video_ingestion container runs on host network namespace, please ensure to kill all the dependency and eta processes running locally on the host. One could run this script to do so `sudo ./docker_setup/kill_local_dependency_eta_processes.sh`. This script is not extensively tested, so please use `ps -ef` command to see there are no locally running dependency and eta processes.

4. **Known Issues**:
    * If one sees `C++ exception` thrown while starting the `ia_video_ingestion` with basler camera, please disconnect the power/lan cable connected to the basler camera and restart ia_video_ingestion by running cmd `docker restart ia_video_ingestion`

## Steps to setup ETA solution on test/factory system

### <u>Configuration</u>
1. All configurable options for ETA goes into [.env](.env) file.
2. All the provisioning related containers goes into [provision-compose.yml](provision-compose.yml)    and ETA containers goes into [docker-compose.yml](docker-compose.yml) 
3. Provide the right value for "CONFIG_FILE" in [.env](.env) file for video source. 
   1. `factory.json` - value to be used if working with defect video files
   2. `factory_prod.json` (default) - value to be used if working with the camera setup
      1. update `factory_prod.json` to use the correct ingestors. [Updating Ingestors](https://github.intel.com/ElephantTrunkArch/ElephantTrunkArch/blob/master/agent/README.md)
    > **Note**: 
    > For AliOS Things OS, we have found that in some of the OS images shared with us, we see the `moby.service` presence and not the `docker.service`. When this is the case, the `eta.service` installation would fail when running `sudo ./setup_eta -a`. Whenever this happens, please change `docker.service` to `moby.service` in `docker_setup/deploy/eta.service` and re-run `sudo ./setup_eta -a` script to install`eta.service` with the changes required.
4. The current trigger for video frame ingestion in ETA stack is happening over MQTT, so mosquitto 
   container can be run on the ECN or a separate node which is configurable.
5. `<Yumei App only>`Follow [YumeiApp/README.md](../YumeiApp/README.md) for ingestion configuration 
   over MQTT, controlling of robotic arm, alarm light and reset button

### <u>Installation</u>
1. Directly building the eta containers from source and starting them using   
   `eta.service` without creating tar balls on the ECN (Edge Compute Node) device 

    - Copy docker_setup files to /opt/intel/eta path and start `eta.service`
        (**scripts   should be executed from `<ElephantTrunkArch>/docker_setup/deploy` directory**)
        
        ```sh
        sudo ./setup_eta -a | tee setup_eta_install_from_source.txt
        ```
        > **Note**: The `eta.service` here will fail to bring up the `ia_data_agent` container as the provisioning is not done

    - Follow below provisioning steps

        1. Certificates generation:

            Follow [cert-tool/README.md](../cert-tool/README.md) to generate the required certificates/keys.

        2. Provision the secrets to Vault (**scripts should be executed from                             `<ElephantTrunkArch>/docker_setup/` directory**)
            
            Run the script:
            ```sh
            provision_startup.sh <PATH_TO_CERTIFICATES_DIRECTORY> | tee provision_startup.txt
                E.g. provision_startup.sh ../cert-tool/Certificates/
            ```
            This will take the inputs from `provision_config.json` & certificates, following which it seal the secrets into the Vault.
            It is responsibility of the Admin to remove the source directory wherein certificates exist.

            **Note**: If the admin wants to update the secrets in the vault, a re-provisioning step needs to be done like below:
            ```sh
            sudo rm -rf /opt/intel/eta/secret_store
            <Update the new values into provision_config.json & custom certificates directory if necessary>
            sudo ./provision_startup.sh <PATH_TO_CERTIFICATES_DIRECTORY> | tee provision_startup.txt
            ```

    - Restart `eta.service` to bring the ETA stack after provisioning

        ```sh
        sudo systemctl restart eta
        ```

2. Follow [VisualHmiClient/README.md](../VisualHmiClient/README.md) to setup Visual HMI client either     natively (due to dependencies, this works only on Ubuntu) or dockerized version.

3. If working with video file i.e, `CONFIG_FILE` is set to `factory.json` in [.env](.env), by          default the video frames are ingested in loop by `ia_video_ingestion` container. One can            also restart ia_video_ingestion container manually by running: 
   `docker restart ia_video_ingestion`

4. If working with basler's camera, need to publish `CAM ON` message over mqtt to  
   `ia_video_ingestion` container by running below command:

    ```sh
    docker exec -it ia_video_ingestion python3.6 mqtt_publish.py
    ```

    **Note**: 
    * While testing with Basler's camera, just provide the right serial number for the camera in [config/factory_prod.json](config/factory_prod.json) under `basler` json field
    * Below scripts provide more control on passing CAM ON and CAM OFF message:
    
    	- Manual way to control Robotic Arm (`camera_state`: 0 for CAM OFF and 1 for CAM ON) - present working dir: `<ElephantTrunkArch>`:
        
          ```sh
          python3.6 VideoIngestion/test/RoboArm_manual.py --camera_state 1
          ```
        - Automatic way to control Robotic Arm by periodically sending CAM ON and CAM OFF message:
        
          ```sh
          python3.6 VideoIngestion/test/RoboArm_auto.py
          ```
        <br/>
 	Check `VideoIngestion/test/config.py` for configuration.

### Post Installation Verification

1. To check if all the ETA images are built successfully, use cmd: **docker images|grep ia** and   
all containers are running, use cmd: **docker ps** (`one should see all the dependency containers and ETA containers up and running`). If you see issues where the build is failing due to non-reachability to Internet, please ensure you have correctly configured proxy settings and restarted docker service. Even after doing this, if you are running into the same issue, please add below instrcutions to all the dockerfiles in `docker_setup\dockerfiles` at the top after the LABEL instruction and retry the building ETA images:
    	
    ```sh
    ENV http_proxy http://proxy.iind.intel.com:911
    ENV https_proxy http://proxy.iind.intel.com:911
    ```

2. All containers in ETA stack:
    * Provisioning containers: vault (`ia_vault`) and provision (`ia_provision`)
    * Dependency containers: log rotate (`ia_log_rotate`) and mosquitto container started separately
    * ETA core containers:  DataAgent (`ia_data_agent`), imagestore (`ia_imagestore`), Video Ingestion (`ia_video_ingestion`),
      Data Analytics (`ia_data_analytics`) and Yumei App (`ia_yumei_app`)

3. Installation directory - `/opt/intel/eta` root directory details:
     * `config/` - all the ETA configs reside here.
     * `logs/` - all the ETA logs reside here. 
     * `dist_libs/` - is the client external libs distribution package
        * `DataAgentClient` -
            * cpp - consists of gRPC cpp client wrappers, protobuff files and example programs
            * py - consists of gRPC py client wrappers, protobuff files and example programs
        * `DataBusAbstraction` - 
            * py - consists of opcua py client wrappers and example programs

### Steps to setup ETA solution on dev system (scripts should be executed from `<ElephantTrunkArch>/docker_setup` directory)</u>

1. Follow provisioning steps in `Steps to setup ETA solution on test/factory system` section 
2. Build and run dependency and ETA images as per the dependency order (one time task unless you change something in Dockerfile of ETA images)
    ```sh
    sudo ./compose_startup.sh | tee compose_startup.txt
    ```

    > **Note**:
    > 1. `compose_startup.sh` script will start the mosquitto container outside of the docker-compose.yml file if there is no local mosquitto service.
    > 2. Please note: `cp -f resolv.conf /etc/resolv.conf` line in `compose_startup.sh` needs to be commented in non-proxy environment before
    starting it off.
    
3. Follow steps# 2, 3 and 4 under `Installation` section of `Steps to setup ETA solution on test/factory system` section

4. Verify `Post Installation Verification` section of `Steps to setup ETA solution on test/factory system` section

5. `<Optional>` This step is an optional step to prepare the ETA as a redistributable .tar image which can be deployed into multiple factory     machines. This will come in handy if the same code need to be deployed in multiple machines. Make sure that you are in        
   **docker_setup/deploy** directory

    ```sh
    sudo ./setup_eta.py -c | tee create_eta_targz.txt
    ```
   **Note**:
   * This step will create an file named `eta.tar.gz` in the "deploy" directory. Copy this tarball to the destination node's preferred directory.
   * Please note `cp -f resolv.conf /etc/resolv.conf` line in `./deploy/deploy_compose_startup.sh` needs to be commented in non-proxy environment before starting it off.

> Note:
1. Few useful docker-compose and docker commands:
     * `docker-compose build` - builds all the service containers. To build a single service container, use `docker-compose build [serv_cont_name]`
     * `docker-compose down` - stops and removes the service containers
     * `docker-compose up -d` - brings up the service containers by picking the changes done in `docker-compose.yml`
     * `docker ps` - check running containers
     * `docker ps -a` - check running and stopped containers
     * `docker stop $(docker ps -a -q)` - stops all the containers
     * `docker rm $(docker ps -a -q)` - removes all the containers. Useful when you run into issue of already container is in use.
     * [docker compose cli](https://docs.docker.com/compose/reference/overview/)
     * [docker compose reference](https://docs.docker.com/compose/compose-file/)
     * [docker cli](https://docs.docker.com/engine/reference/commandline/cli/#configuration-files)
2. If you want to run the docker images separately i.e, one by one, run the command `docker-compose    run --no-deps [service_cont_name]` Eg: `docker-compose run --name ia_video_ingestion --no-deps      ia_video_ingestion` to run VI container and the switch `--no-deps` will not bring up it's        
   dependencies mentioned in the docker-compose file. If the container is not launching, there could be some issue with entrypoint program which could be overrided by providing this extra switch `--entrypoint /bin/bash` before the service container name in the docker-compose run command above, this would let one inside the container and run the actual entrypoint program from the container's terminal to rootcause the issue. If the container is running and one wants to get inside, use cmd: `docker-compose exec [service_cont_name] /bin/bash` or `docker exec -it [cont_name] /bin/bash`
3. For debug purpose, it becomes essential to send dev team the logs of the build/run scripts to       rootcause the issue effectively. This is where the `tee` command comes to rescue.
4. Best way to check logs of containers is to use command: `docker logs -f [cont_name]`. If one        wants to see all the docker-compose service container logs at once, then just run        
   `docker-compose logs -f`
